\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\usepackage{indentfirst} % Indent first paragraph after section

\title{Graphs and Networks in Artificial Intelligence}
\author{Denys Mykhailov}

\begin{document}
\maketitle

\begin{abstract}

      This essay explores the intersection of graph theory and artificial intelligence, focusing on the development and application of Graph Neural Networks (GNNs).
      It traces the historical roots of graph-based AI, discusses key theoretical foundations, and highlights major developments such as Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs).
      The essay also examines current applications of GNNs in various domains, influential contributors to the field, and open challenges for future research.
      By understanding the evolution and capabilities of GNNs, we gain insights into how these models can effectively learn from graph-structured data, enabling advancements in AI systems that leverage relational information.

\end{abstract}

\section{Introduction}

Graphs are mathematical structures used to model pairwise relations between entities.
A graph consists of \textbf{nodes} (vertices) representing entities and \textbf{edges} representing connections or relationships between these entities.
Due to their flexibility in representing complex systems, graphs appear in countless real-world scenarios – from social networks linking people, to molecules with atoms connected by chemical bonds, to knowledge graphs encoding facts.
In the field of Artificial Intelligence (AI), graphs have long been used as a natural way to represent knowledge and structure (for example, \textbf{semantic networks} in early AI represented knowledge as nodes and links).
However, only recently have we developed powerful learning algorithms, called \textbf{Graph Neural Networks (GNNs)}, that can directly learn from graph-structured data rather than just using graphs as static data structures.
These GNNs extend classical neural networks to work on arbitrary graphs, enabling AI systems to better exploit \textbf{relational information}.

The rise of GNNs represents a convergence of graph theory (a branch of discrete mathematics dating back to the 18th century) and modern deep learning.
In this essay, we explore the origins of this interdisciplinary field, its key theoretical foundations and results, major developments (such as GCNs and GANs), current applications of graph-based AI, influential contributors, and open challenges for the future.

\subsection{Origins of Graph Theory and Early Graph Applications in AI}

\textbf{Graph theory} as a mathematical field originated from the famous Königsberg bridge problem solved by Leonhard Euler in 1736 \cite{carlson2019konigsberg}.
Euler’s solution – proving that no path exists that crosses each of the city’s seven bridges exactly once – is regarded as the first theorem in graph theory and marked the birth of graph theory as a discipline.
Over the next two centuries, graph theory was developed by many mathematicians, including Cayley, Kirchhoff, and Whitney, who established foundational concepts such as connectivity, planarity, and graph coloring.

% photo of Königsberg bridges
\begin{figure}[ht]
      \centering
      \includegraphics[width=0.4\textwidth]{../assets/konigsberg-bridges.png}
      \caption{The original seven bridges of Königsberg, which inspired Euler's work on graph theory.}
      \label{fig:konigsberg-bridges}
\end{figure}

\subsection{Early Graph Applications in AI}

In the early days of AI, graphs were primarily used to represent knowledge, relationships and reasoning.
Early AI pioneers like Marvin Minsky and others introduced \textbf{semantic networks} (graph-based knowledge representations) in the 1960s and 1970s to encode relationships between concepts in natural language understanding \cite{kelemen2007neural}.

Around the same time, probabilistic graphical models were developed – \textbf{Bayesian networks} introduced by Judea Pearl in the 1980s \cite{pearl1995bayesian} and \textbf{Markov random fields} \cite{lang2024abstract} – which leveraged graph structures (directed or undirected graphs) to represent probabilistic dependencies among variables for AI reasoning using joint \textbf{probability distributions} and \textbf{factor graphs} \cite{loeliger2004factor}.

These early uses of graphs in AI, however, treated graphs as static structures or as a backdrop for algorithms; there was \textit{no concept} of learning on graphs with neural networks yet.

% photo of Bayesian networks and Markov random fields
\begin{figure}[ht]
      \centering
      \includegraphics[width=1\textwidth,trim=0 100 0 0,clip]{../assets/bayesian-and-markov-networks.png}
      \caption{\textbf{Minimal I-map Bayesian networks for a nonchordal Markov network.} \\ (a) A Markov network $\mathcal{H}_\ell$ with a loop. (b) A minimal I-map $\mathcal{G}_\ell$ Bayesian network for $\mathcal{H}_\ell$.}
      \label{fig:bayesian-networks}
\end{figure}

\subsection{The Emergence of Graph Neural Networks}

The idea of combining neural networks with graph-structured data began to take shape in the 1990s.
Researchers explored recursive neural networks (RNNs) \cite{schmidt2019recurrent} and other architectures that could process structured inputs like sequences or trees.
Notably, Frasconi, Gori, and Sperduti (1998) \cite{frasconi1998general} proposed a \textbf{general framework} for adapting neural networks to structured data like graphs.
These early approaches could handle specific cases (often restricted to trees or acyclic graphs) by \textbf{recursively propagating information}, but they were \textit{limited} in scope and difficult to train.

A milestone was reached in 2007, when \textbf{Marco Gori} and colleagues introduced \textit{a new model for learning in graph domains} \cite{gori2007new} – essentially the first design of what we now call a \textbf{Graph Neural Network (GNN)}.

This was followed by \textit{Franco Scarselli}, \textit{Marco Gori}, and others formally defining the Graph Neural Network (GNN) model in 2009 \cite{scarselli2009graph}.
Scarselli et al.’s GNN was a neural network architecture that could directly take a graph as input and perform inference either at the node level or graph level, by iteratively propagating "state" information along the graph’s edges until reaching a stable equilibrium.
This model extended earlier recursive neural net ideas and incorporated concepts from \textbf{random walks} and \textbf{Markov chains} on graphs.

In short, the GNN model of 2009 provided a way to learn node representations by an iterative \textbf{message-passing process} on the graph \cite{rizvi2022fimp}.
Early applications of these first GNNs included predicting properties of chemical compounds, classifying web pages (by modeling the web as a graph), fraud detection and other relational tasks.
The field was thus born from a marriage of graph theory (to represent relational structure) and neural network learning.

% photo of GNN message passing
\begin{figure}[ht]
      \centering
      \includegraphics[width=0.8\textwidth]{../assets/gnn-message-passing.png}
      \caption{Message passing in Graph Neural Networks (GNNs).
            The GNN iteratively updates node representations by aggregating information from neighboring nodes, allowing it to learn complex relationships in graph-structured data.}
      \label{fig:gnn-message-passing}
\end{figure}


\bibliographystyle{plain}
\bibliography{references}

\end{document}
